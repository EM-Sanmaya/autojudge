{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1uUHCabAxT42zGzbLK-ZDwq0QqiQ5sQr0",
      "authorship_tag": "ABX9TyMDBcewWVsWVaPIjcV5e7mz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EM-Sanmaya/autojudge/blob/main/Copy_of_AutoJudge_Training_Reference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# ROBUST REGRESSION: CLASS-CONDITIONAL LINEAR SVR\n",
        "# =====================================================\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.svm import LinearSVR\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Reuse features from classification\n",
        "X = X_selected                     # already built, stable features\n",
        "y_score = df[\"problem_score\"].values\n",
        "y_class = df[\"problem_class\"].values\n",
        "\n",
        "# Train-test split (same seed for fairness)\n",
        "idx = np.arange(len(y_score))\n",
        "train_idx, test_idx = train_test_split(\n",
        "    idx,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_class\n",
        ")\n",
        "\n",
        "X_train, X_test = X[train_idx], X[test_idx]\n",
        "y_train_score, y_test_score = y_score[train_idx], y_score[test_idx]\n",
        "y_train_class, y_test_class = y_class[train_idx], y_class[test_idx]\n",
        "\n",
        "# Containers\n",
        "y_pred = np.zeros_like(y_test_score)\n",
        "\n",
        "# ----------------------------\n",
        "# Train separate regressors\n",
        "# ----------------------------\n",
        "for cls in [\"easy\", \"medium\", \"hard\"]:\n",
        "    mask_train = (y_train_class == cls)\n",
        "    mask_test  = (y_test_class == cls)\n",
        "\n",
        "    if np.sum(mask_train) < 20:\n",
        "        continue\n",
        "\n",
        "    svr = LinearSVR(\n",
        "        C=5.0,\n",
        "        epsilon=0.1,\n",
        "        max_iter=5000,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    svr.fit(X_train[mask_train], y_train_score[mask_train])\n",
        "    y_pred[mask_test] = svr.predict(X_test[mask_test])\n",
        "\n",
        "# ----------------------------\n",
        "# Evaluation\n",
        "# ----------------------------\n",
        "mae = mean_absolute_error(y_test_score, y_pred)\n",
        "rmse = np.sqrt(mean_squared_error(y_test_score, y_pred))\n",
        "r2 = r2_score(y_test_score, y_pred)\n",
        "\n",
        "print(\"CLASS-CONDITIONAL REGRESSION RESULTS:\")\n",
        "print(f\"MAE  : {mae:.3f}\")\n",
        "print(f\"RMSE : {rmse:.3f}\")\n",
        "print(f\"RÂ²   : {r2:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBKFdORtQrNG",
        "outputId": "93bfd6f4-57cf-4c08-f6d4-751b9f01653e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLASS-CONDITIONAL REGRESSION RESULTS:\n",
            "MAE  : 0.793\n",
            "RMSE : 1.010\n",
            "RÂ²   : 0.790\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# EASY-AWARE FEATURE ENGINEERING + COST-SENSITIVE HIERARCHICAL SVM\n",
        "# =====================================================\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# -------------------------------\n",
        "# 1. TF-IDF FEATURES\n",
        "# -------------------------------\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=12000,\n",
        "    ngram_range=(1, 2),\n",
        "    stop_words=\"english\"\n",
        ")\n",
        "\n",
        "X_tfidf = tfidf.fit_transform(df[\"combined_text\"])\n",
        "\n",
        "# -------------------------------\n",
        "# 2. FEATURE EXTRACTION (HINTS + EASY SIGNALS)\n",
        "# -------------------------------\n",
        "\n",
        "ALGO_KEYWORDS = [\n",
        "    \"dp\", \"dynamic programming\", \"graph\", \"dfs\", \"bfs\", \"tree\",\n",
        "    \"segment tree\", \"bitmask\", \"mask\", \"flow\", \"geometry\",\n",
        "    \"binary search\", \"greedy\", \"recursion\"\n",
        "]\n",
        "\n",
        "def extract_features(text):\n",
        "    text = text.lower()\n",
        "    words = text.split()\n",
        "\n",
        "    # ----- Basic (from hints) -----\n",
        "    text_length = len(text)\n",
        "    math_symbols = len(re.findall(r\"[+\\-*/%=<>]\", text))\n",
        "    keyword_freq = sum(text.count(k) for k in ALGO_KEYWORDS)\n",
        "\n",
        "    # ----- Easy-specific simplicity signals -----\n",
        "    avg_word_len = np.mean([len(w) for w in words]) if words else 0\n",
        "    sentence_count = text.count(\".\")\n",
        "    has_algorithm = int(keyword_freq > 0)\n",
        "\n",
        "    # ----- Constraint magnitude -----\n",
        "    numbers = [int(n) for n in re.findall(r\"\\d+\", text)]\n",
        "    max_constraint = max(numbers) if numbers else 0\n",
        "\n",
        "    # ----- Control-flow density -----\n",
        "    control_tokens = len(re.findall(r\"\\b(if|for|while)\\b\", text))\n",
        "\n",
        "    return [\n",
        "        text_length,\n",
        "        math_symbols,\n",
        "        keyword_freq,\n",
        "        avg_word_len,\n",
        "        sentence_count,\n",
        "        has_algorithm,\n",
        "        max_constraint,\n",
        "        control_tokens\n",
        "    ]\n",
        "\n",
        "extra_features = np.array([extract_features(t) for t in df[\"combined_text\"]])\n",
        "\n",
        "# -------------------------------\n",
        "# 3. SCALE (chiÂ² safe)\n",
        "# -------------------------------\n",
        "scaler = MinMaxScaler()\n",
        "extra_scaled = scaler.fit_transform(extra_features)\n",
        "\n",
        "# -------------------------------\n",
        "# 4. COMBINE FEATURES\n",
        "# -------------------------------\n",
        "X_full = hstack([X_tfidf, extra_scaled])\n",
        "print(\"Combined feature shape:\", X_full.shape)\n",
        "\n",
        "# -------------------------------\n",
        "# 5. FEATURE SELECTION\n",
        "# -------------------------------\n",
        "selector = SelectKBest(chi2, k=5000)\n",
        "X_selected = selector.fit_transform(X_full, df[\"problem_class\"])\n",
        "print(\"Selected feature shape:\", X_selected.shape)\n",
        "\n",
        "y = df[\"problem_class\"].values\n",
        "\n",
        "# =====================================================\n",
        "# HIERARCHICAL CLASSIFICATION (EASY-FOCUSED)\n",
        "# =====================================================\n",
        "\n",
        "# -------- Stage 1: Easy vs Non-Easy (COST-SENSITIVE) --------\n",
        "y_stage1 = np.where(y == \"easy\", \"easy\", \"non_easy\")\n",
        "\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(\n",
        "    X_selected,\n",
        "    y_stage1,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_stage1\n",
        ")\n",
        "\n",
        "stage1 = LinearSVC(\n",
        "    class_weight={\"easy\": 2.5, \"non_easy\": 1.0},  # ðŸ”´ Boost Easy\n",
        "    C=2.5,\n",
        "    max_iter=8000\n",
        ")\n",
        "\n",
        "stage1.fit(X1_train, y1_train)\n",
        "y1_pred = stage1.predict(X1_test)\n",
        "\n",
        "print(\"\\nStage-1 Accuracy (Easy vs Non-Easy):\",\n",
        "      accuracy_score(y1_test, y1_pred))\n",
        "\n",
        "# -------- Stage 2: Medium vs Hard --------\n",
        "mask = (y != \"easy\")\n",
        "X_stage2 = X_selected[mask]\n",
        "y_stage2 = y[mask]\n",
        "\n",
        "X2_train, X2_test, y2_train, y2_test = train_test_split(\n",
        "    X_stage2,\n",
        "    y_stage2,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_stage2\n",
        ")\n",
        "\n",
        "stage2 = LinearSVC(\n",
        "    class_weight=\"balanced\",\n",
        "    C=2.5,\n",
        "    max_iter=8000\n",
        ")\n",
        "\n",
        "stage2.fit(X2_train, y2_train)\n",
        "\n",
        "# -------- Final Combined Prediction --------\n",
        "_, X_final_test, _, y_final_true = train_test_split(\n",
        "    X_selected,\n",
        "    y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "final_preds = []\n",
        "\n",
        "for i in range(X_final_test.shape[0]):\n",
        "    if stage1.predict(X_final_test[i])[0] == \"easy\":\n",
        "        final_preds.append(\"easy\")\n",
        "    else:\n",
        "        final_preds.append(stage2.predict(X_final_test[i])[0])\n",
        "\n",
        "print(\"\\nFINAL HIERARCHICAL ACCURACY:\",\n",
        "      accuracy_score(y_final_true, final_preds))\n",
        "\n",
        "print(\"\\nFINAL CLASSIFICATION REPORT:\")\n",
        "print(classification_report(y_final_true, final_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc6zIaenRhmg",
        "outputId": "d53e9874-ae4b-49b2-8594-9faa9e86103a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined feature shape: (4112, 12008)\n",
            "Selected feature shape: (4112, 5000)\n",
            "\n",
            "Stage-1 Accuracy (Easy vs Non-Easy): 0.8080194410692588\n",
            "\n",
            "FINAL HIERARCHICAL ACCURACY: 0.8031591737545565\n",
            "\n",
            "FINAL CLASSIFICATION REPORT:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        easy       0.65      0.41      0.50       153\n",
            "        hard       0.85      0.90      0.87       389\n",
            "      medium       0.79      0.89      0.83       281\n",
            "\n",
            "    accuracy                           0.80       823\n",
            "   macro avg       0.76      0.73      0.74       823\n",
            "weighted avg       0.79      0.80      0.79       823\n",
            "\n"
          ]
        }
      ]
    }
  ]
}